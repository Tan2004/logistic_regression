{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.jupyter.org/github/tfolkman/learningwithdata/blob/master/Logistic%20Gradient%20Descent.ipynb\n",
    "    \n",
    "    Logistic regression is an excellent tool to know for classification problems. Classification problems are problems where you are trying to classify observations into groups. To make our examples more concrete, we will consider the Iris dataset. The iris dataset contains 4 attributes for 3 types of iris plants. The purpose is to classify which plant you have just based on the attributes. To simplify things, we will only consider 2 attributes and 2 classes. Here are the data visually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAERCAYAAABowZDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRA5hwiAiIof8PzKvaim71LJSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2626585f2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "data = datasets.load_iris()\n",
    "X = data.data[:100, :2]\n",
    "y = data.target[:100]\n",
    "X_full = data.data[:100, :]\n",
    "\n",
    "setosa = plt.scatter(X[:50,0], X[:50,1], c='b')\n",
    "versicolor = plt.scatter(X[50:,0], X[50:,1], c='r')\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.legend((setosa, versicolor), (\"Setosa\", \"Versicolor\"))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wow! \n",
    "This is nice - the two classes are completely separate. Now this obviously is a toy example, but let's now think about how to create a learning algorithm to give us the probability that given Sepal Width and Sepal Length the plant is Setosa. So if our algorithm returns .9 we place 90% probability on the plant being Setosa and 10% probability on it being Versicolor.\n",
    "Logisitic Function\n",
    "\n",
    "So we want to return a value between 0 and 1 to make sure we are actually representing a probability. To do this we will make use of the logistic function. The logistic function mathematically looks like this:\n",
    "y=11+e−x\n",
    "\n",
    "Let's take a look at the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEDCAYAAAAyZm/jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26263e5bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = np.linspace(-5, 5, 100)\n",
    "y_values = [1 / (1 + math.e**(-x)) for x in x_values]\n",
    "plt.plot(x_values, y_values)\n",
    "plt.axhline(.5)\n",
    "plt.axvline(0)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see why this is a great function for a probability measure. The y-value represents the probability and only ranges between 0 and 1. Also, for an x value of zero you get a .5 probability and as you get more positive x values you get a higher probability and more negative x values a lower probability.\n",
    "Make use of your data\n",
    "\n",
    "Okay - so this is nice, but how the heck do we use it? Well we know we have two attributes - Sepal length and Sepal width - that we need to somehow use in our logistic function. One pretty obvious thing we could do is:\n",
    "\n",
    "x=β0+β1SW+β2SL\n",
    "\n",
    "Where SW is our value for sepal width and SL is our value for sepal length. For those of you familiar with Linear Regression this looks very familiar. Basically we are assuming that x is a linear combination of our data plus an intercept. For example, say we have a plant with a sepal width of 3.5 and a sepal length of 5 and some oracle tells us that β0=1\n",
    ", β1=2, and β2=4\n",
    "\n",
    ". This would imply:\n",
    "\n",
    "x=1+(2∗3.5)+(4∗5)=28\n",
    "\n",
    "Plugging this into our logistic function gives:\n",
    "\n",
    "11+e−28=.99\n",
    "\n",
    "So we would give a 99% probability to a plant with those dimensions as being Setosa.\n",
    "Learning\n",
    "\n",
    "Okay - makes sense. But who is this oracle giving us our β\n",
    "values? Good question! This is where the learning in machine learning comes in :). We will learn our β\n",
    "\n",
    "values.\n",
    "Step 1 - Define your cost function\n",
    "\n",
    "If you have been around machine learning, you probably hear the phrase \"cost function\" thrown around. Before we get to that, though, let's do some thinking. We are trying to choose β\n",
    "values in order to maximize the probability of correctly classifying our plants. That is just the definition of our problem. Let's say someone did give us some β\n",
    "\n",
    "values, how would we determine if they were good values or not? We saw above how to get the probability for one example. Now imagine we did this for all our plant observations - all 100. We would now have 100 probability scores. What we would hope is that for the Setosa plants, the probability values are close to 1 and for the Versicolor plants the probability is close to 0.\n",
    "\n",
    "But we don't care about getting the correct probability for just one observation, we want to correctly classify all our observations. If we assume our data are independent and identically distributed, we can just take the product of all our individually calculated probabilities and that is the value we want to maximize. So in math:\n",
    "∏Setosa11+e−(β0+β1SW+β2SL)∏Versicolor1−11+e−(β0+β1SW+β2SL)\n",
    "If we define the logistic function as:\n",
    "h(x)=11+e−x\n",
    "and x as:\n",
    "x=β0+β1SW+β2SL\n",
    "This can be simplified to:\n",
    "∏Setosah(x)∏Versicolor1−h(x)\n",
    "\n",
    "The ∏\n",
    "symbol means take the product for the observations classified as that plant. Here we are making use of the fact that are data are labeled, so this is called supervised learning. Also, you will notice that for Versicolor observations we are taking 1 minus the logistic function. That is because we are trying to find a value to maximize, and since Versicolor observations should have a probability close to zero, 1 minus the probability should be close to 1. So now we know that we want to maximize the following:\n",
    "∏Setosah(x)∏Versicolor1−h(x)\n",
    "\n",
    "So we now have a value we are trying to maximize. Typically people switch this to minimization by making it negative:\n",
    "−∏Setosah(x)∏Versicolor1−h(x)\n",
    "\n",
    "Note: minimizing the negative is the same as maximizing the positive. The above formula would be called our cost function.\n",
    "Step 2 - Gradients\n",
    "\n",
    "So now we have a value to minimize, but how do we actually find the β\n",
    "\n",
    "values that minimize our cost function? Do we just try a bunch? That doesn't seem like a good idea...\n",
    "\n",
    "This is where convex optimization comes into play. We know that the logistic cost function is convex - just trust me on this. And since it is convex, it has a single global minimum which we can converge to using gradient descent.\n",
    "\n",
    "Here is an image of a convex function:\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url=\"http://www.me.utexas.edu/~jensen/ORMM/models/unit/nonlinear/subunits/terminology/graphics/convex1.gif\")\n",
    "\n",
    "Now you can imagine, that this curve is our cost function defined above and that if we just pick a point on the curve, and then follow it down to the minimum we would eventually reach the minimum, which is our goal. Here is an animation of that. That is the idea behind gradient descent.\n",
    "\n",
    "So the way we follow the curve is by calculating the gradients or the first derivatives of the cost function with respect to each β\n",
    "\n",
    ". So lets do some math. First realize that we can also define the cost function as:\n",
    "\n",
    "−∑i=1100yilog(h(xi))+(1−yi)log(1−h(xi))\n",
    "\n",
    "This is because when we take the log our product becomes a sum. See log rules. And if we define yi\n",
    "to be 1 when the observation is Setosa and 0 when Versicolor, then we only do h(x) for Setosa and 1 - h(x) for Versicolor. So lets take the derivative of this new version of our cost function with respect to β0. Remember that our β0 is in our x value. So remember that the derivative of log(x) is 1x\n",
    "\n",
    ", so we get (for each observation):\n",
    "\n",
    "yih(xi)+1−yi1−h(xi)\n",
    "\n",
    "And using the quotient rule we see that the derivative of h(x) is:\n",
    "\n",
    "e−x(1+e−x)2=11+e−x(1−11+e−x)=h(x)(1−h(x))\n",
    "\n",
    "And the derivative of x with respect to β0\n",
    "\n",
    "is just 1. Putting it all together we get:\n",
    "\n",
    "yih(xi)(1−h(xi))h(xi)−(1−yi)h(xi)(1−h(xi))1−h(xi)\n",
    "\n",
    "Simplify to:\n",
    "\n",
    "yi(1−h(xi))−(1−yi)h(xi)=yi−yih(xi)−h(xi)+yih(xi)=yi−h(xi)\n",
    "\n",
    "Bring in the neative and sum and we get the partial derivative with respect to β0\n",
    "\n",
    "to be:\n",
    "\n",
    "∑i=1100h(xi)−yi\n",
    "\n",
    "Now the other partial derivaties are easy. The only change is now the derivative for xi\n",
    "is no longer 1. For β1 it is SWi and for β2 it is SLi. So the partial derivative for β1\n",
    "\n",
    "is:\n",
    "\n",
    "∑i=1100(h(xi)−yi)SWi\n",
    "\n",
    "For β2\n",
    "\n",
    ":\n",
    "\n",
    "∑i=1100(h(xi)−yi)SLi\n",
    "\n",
    "Step 3 - Gradient Descent\n",
    "\n",
    "So now that we have our gradients, we can use the gradient descent algorithm to find the values for our β\n",
    "\n",
    "s that minimize our cost function. The gradient descent algorithm is very simple:\n",
    "\n",
    "    Initially guess any values for your β\n",
    "\n",
    "values\n",
    "Repeat until converge:\n",
    "\n",
    "    βi=βi−(α∗\n",
    "\n",
    "gradient with respect to βi) for i=0,1,2\n",
    "\n",
    "        in our case\n",
    "\n",
    "Here α\n",
    "is our learning rate. Basically how large of steps to take on our cost curve. What we are doing is taking our current β value and then subtracting some fraction of the gradient. We subtract because the gradient is the direction of greatest increase, but we want the direction of greatest decrease, so we subtract. In other words, we pick a random point on our cost curve, check to see which direction we need to go to get closer to the minimum by using the negative of the gradient, and then update our β values to move closer to the minimum. Repeat until converge means keep updating our β values until our cost value converges - or stops decreasing - meaning we have reached the minimum. Also, it is important to update all the β values at the same time. Meaning that you use the same previous β values to update all the next β\n",
    "\n",
    "values.\n",
    "Gradient Descent Tricks\n",
    "\n",
    "I think most of this are from Andrew Ng's machine learning course\n",
    "\n",
    "    Normalize variables:\n",
    "        This means for each variable subtract the mean and divide by standard deviation.\n",
    "    Learning rate:\n",
    "        If not converging, the learning rate needs to be smaller - but will take longer to converge\n",
    "        Good values to try ..., .001, .003, .01, .03, .1, .3, 1, 3, ...\n",
    "    Declare converges if cost decreases by less than 10−3\n",
    "\n",
    "    (this is just a decent suggestion)\n",
    "    Plot convergence as a check\n",
    "\n",
    "Lets see some code\n",
    "\n",
    "Below is code that implements everything we discussed. It is vectorized, though, so things are represented as vectors and matricies. It should still be fairly clear what is going on (I hope...if not, please let me know and I can put out a version closer to the math). Also, I didn't implement an intercept (so no β0\n",
    "\n",
    ") feel free to add this if you wish :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_func(theta, x):\n",
    "    return float(1) / (1 + math.e**(-x.dot(theta)))\n",
    "\n",
    "def log_gradient(theta, x, y):\n",
    "    first_calc = logistic_func(theta, x) - np.squeeze(y)\n",
    "    final_calc = first_calc.T.dot(x)\n",
    "    return final_calc\n",
    "\n",
    "def cost_func(theta, x, y):\n",
    "    log_func_v = logistic_func(theta,x)\n",
    "    y = np.squeeze(y)\n",
    "    step1 = y * np.log(log_func_v)\n",
    "    step2 = (1-y) * np.log(1 - log_func_v)\n",
    "    final = -step1 - step2\n",
    "    return np.mean(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_desc(theta_values, X, y, lr=.001, converge_change=.001):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    #setup cost iter\n",
    "    cost_iter = []\n",
    "    cost = cost_func(theta_values, X, y)\n",
    "    cost_iter.append([0, cost])\n",
    "    change_cost = 1\n",
    "    i = 1\n",
    "    while(change_cost > converge_change):\n",
    "        old_cost = cost\n",
    "        theta_values = theta_values - (lr * log_gradient(theta_values, X, y))\n",
    "        cost = cost_func(theta_values, X, y)\n",
    "        cost_iter.append([i, cost])\n",
    "        change_cost = old_cost - cost\n",
    "        i+=1\n",
    "    return theta_values, np.array(cost_iter)\n",
    "\n",
    "def pred_values(theta, X, hard=True):\n",
    "    #normalize\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    pred_prob = logistic_func(theta, X)\n",
    "    pred_value = np.where(pred_prob >= .5, 1, 0)\n",
    "    if hard:\n",
    "        return pred_value\n",
    "    return pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas= [ 0.2  0.3]\n",
      "X.dot(betas).dtype= float64\n",
      "X.dot(betas)= [ 2.07  1.88  1.9   1.85  2.08  2.25  1.94  2.02  1.75  1.91  2.19  1.98\n",
      "  1.86  1.76  2.36  2.46  2.25  2.07  2.28  2.16  2.1   2.13  2.    2.01\n",
      "  1.98  1.9   2.02  2.09  2.06  1.9   1.89  2.1   2.27  2.36  1.91  1.96\n",
      "  2.15  1.91  1.78  2.04  2.05  1.59  1.84  2.05  2.16  1.86  2.16  1.88\n",
      "  2.17  1.99  2.36  2.24  2.31  1.79  2.14  1.98  2.25  1.7   2.19  1.85\n",
      "  1.6   2.08  1.86  2.09  1.99  2.27  2.02  1.97  1.9   1.87  2.14  2.06\n",
      "  2.01  2.06  2.15  2.22  2.2   2.24  2.07  1.92  1.82  1.82  1.97  2.01\n",
      "  1.98  2.22  2.27  1.95  2.02  1.85  1.88  2.12  1.94  1.69  1.93  2.04\n",
      "  2.01  2.11  1.77  1.98]\n",
      "X= [[ 5.1  3.5]\n",
      " [ 4.9  3. ]\n",
      " [ 4.7  3.2]\n",
      " [ 4.6  3.1]\n",
      " [ 5.   3.6]]\n",
      "X.shape= (100, 2)\n",
      "L= [ 0.88795296  0.86761113  0.86989153  0.8641271   0.88894403  0.90465054\n",
      "  0.87435214  0.88288101  0.8519528   0.87101915  0.89934791  0.87868116\n",
      "  0.86529695  0.85320966  0.91372581  0.92128966  0.90465054  0.88795296\n",
      "  0.90720705  0.89659955  0.89090318  0.89378501  0.88079708  0.88184302\n",
      "  0.87868116  0.86989153  0.88288101  0.88992743  0.88695417  0.86989153\n",
      "  0.86875553  0.89090318  0.90636179  0.91372581  0.87101915  0.87653295\n",
      "  0.89566878  0.87101915  0.85569687  0.88493327  0.88594762  0.8306161\n",
      "  0.86294871  0.88594762  0.89659955  0.86529695  0.89659955  0.86761113\n",
      "  0.89752297  0.87974314  0.91372581  0.90378446  0.90970186  0.85692728\n",
      "  0.89473061  0.87868116  0.90465054  0.84553473  0.89934791  0.8641271\n",
      "  0.83201839  0.88894403  0.86529695  0.88992743  0.87974314  0.90636179\n",
      "  0.88288101  0.87761111  0.86989153  0.86645828  0.89473061  0.88695417\n",
      "  0.88184302  0.88695417  0.89566878  0.9020312   0.90024951  0.90378446\n",
      "  0.88795296  0.87213843  0.86056613  0.86056613  0.87761111  0.88184302\n",
      "  0.87868116  0.9020312   0.90636179  0.87544664  0.88288101  0.8641271\n",
      "  0.86761113  0.89283193  0.87435214  0.84422416  0.87324942  0.88493327\n",
      "  0.88184302  0.89187133  0.85445767  0.87868116]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "[ 0.88795296  0.86761113  0.86989153  0.8641271   0.88894403  0.90465054\n",
      "  0.87435214  0.88288101  0.8519528   0.87101915  0.89934791  0.87868116\n",
      "  0.86529695  0.85320966  0.91372581  0.92128966  0.90465054  0.88795296\n",
      "  0.90720705  0.89659955  0.89090318  0.89378501  0.88079708  0.88184302\n",
      "  0.87868116  0.86989153  0.88288101  0.88992743  0.88695417  0.86989153\n",
      "  0.86875553  0.89090318  0.90636179  0.91372581  0.87101915  0.87653295\n",
      "  0.89566878  0.87101915  0.85569687  0.88493327  0.88594762  0.8306161\n",
      "  0.86294871  0.88594762  0.89659955  0.86529695  0.89659955  0.86761113\n",
      "  0.89752297  0.87974314  0.91372581  0.90378446  0.90970186  0.85692728\n",
      "  0.89473061  0.87868116  0.90465054  0.84553473  0.89934791  0.8641271\n",
      "  0.83201839  0.88894403  0.86529695  0.88992743  0.87974314  0.90636179\n",
      "  0.88288101  0.87761111  0.86989153  0.86645828  0.89473061  0.88695417\n",
      "  0.88184302  0.88695417  0.89566878  0.9020312   0.90024951  0.90378446\n",
      "  0.88795296  0.87213843  0.86056613  0.86056613  0.87761111  0.88184302\n",
      "  0.87868116  0.9020312   0.90636179  0.87544664  0.88288101  0.8641271\n",
      "  0.86761113  0.89283193  0.87435214  0.84422416  0.87324942  0.88493327\n",
      "  0.88184302  0.89187133  0.85445767  0.87868116]\n"
     ]
    }
   ],
   "source": [
    "shape = X.shape[1]\n",
    "y_flip = np.logical_not(y) #flip Setosa to be 1 and Versicolor to zero to be consistent\n",
    "betas =np.array( [0.2,0.3]) #np.zeros(shape)\n",
    "L=logistic_func(betas, X)\n",
    "#G= log_gradient(betas, X, y_flip )\n",
    "print('betas=', betas)\n",
    "print('X.dot(betas).dtype=',X.dot(betas).dtype)\n",
    "print('X.dot(betas)=',X.dot(betas))\n",
    "#print('betas.dot(X)=',betas.dot(X)\n",
    "print('X=',X[0:5,])\n",
    "print('X.shape=',X.shape)\n",
    "print('L=',L)\n",
    "print(y_flip)\n",
    "print(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.07,  1.88,  1.9 ,  1.85,  2.08,  2.25,  1.94,  2.02,  1.75,\n",
       "        1.91,  2.19,  1.98,  1.86,  1.76,  2.36,  2.46,  2.25,  2.07,\n",
       "        2.28,  2.16,  2.1 ,  2.13,  2.  ,  2.01,  1.98,  1.9 ,  2.02,\n",
       "        2.09,  2.06,  1.9 ,  1.89,  2.1 ,  2.27,  2.36,  1.91,  1.96,\n",
       "        2.15,  1.91,  1.78,  2.04,  2.05,  1.59,  1.84,  2.05,  2.16,\n",
       "        1.86,  2.16,  1.88,  2.17,  1.99,  2.36,  2.24,  2.31,  1.79,\n",
       "        2.14,  1.98,  2.25,  1.7 ,  2.19,  1.85,  1.6 ,  2.08,  1.86,\n",
       "        2.09,  1.99,  2.27,  2.02,  1.97,  1.9 ,  1.87,  2.14,  2.06,\n",
       "        2.01,  2.06,  2.15,  2.22,  2.2 ,  2.24,  2.07,  1.92,  1.82,\n",
       "        1.82,  1.97,  2.01,  1.98,  2.22,  2.27,  1.95,  2.02,  1.85,\n",
       "        1.88,  2.12,  1.94,  1.69,  1.93,  2.04,  2.01,  2.11,  1.77,  1.98])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(betas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it to the test\n",
    "\n",
    "So here I will use the above code for our toy example. I initalize our β\n",
    "values to all be zero, then run gradient descent to learn the β\n",
    "\n",
    "values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.52645347  1.39922382]\n"
     ]
    }
   ],
   "source": [
    "shape = X.shape[1]\n",
    "y_flip = np.logical_not(y) #flip Setosa to be 1 and Versicolor to zero to be consistent\n",
    "betas = np.zeros(shape)\n",
    "fitted_values, cost_iter = grad_desc(betas, X, y_flip)\n",
    "print(fitted_values)\n",
    "\n",
    "#[-1.52645347  1.39922382]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I get a value of -1.5 for β1\n",
    "and a value of 1.4 for β2. Remember that β1 is my coefficient for Sepal Length and β2\n",
    "\n",
    "for Sepal Width. Meaning that as sepal width becomes larger I would have a stronger prediction for Setosa and as Sepal Length becomes larger I have more confidence it the plant being Versicolor. Which makes sense when looking at our earlier plot.\n",
    "\n",
    "Now let's make some predictions (Note: since we are returning a probability, if the probability is greater than or equal to 50% then I assign the value to Setosa - or a value of 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y = pred_values(fitted_values, X)\n",
    "predicted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-c8eb451c9705>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-c8eb451c9705>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    And let's see how accurate we are:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "And let's see how accurate we are:\n",
    "\n",
    "np.sum(y_flip == predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cool - we got all but 1 right. So that is pretty good. But again note: this is a very simple example, where getting all correct is actually pretty easy and we are looking at training accuracy. But that is not the point - we just want to make sure our algorithm is working.\n",
    "\n",
    "We can do another check by taking a look at how our gradient descent converged:\n",
    "\n",
    "plt.plot(cost_iter[:,0], cost_iter[:,1])\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "sns.despine()\n",
    "\n",
    "You can see that as we ran our algorithm, we continued to decrease our cost function and we stopped right at about when we see the decrease in cost to level out. Nice - everything seems to be working!\n",
    "\n",
    "Lastly, another nice check is to see how well a packaged version of the algorithm does:\n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X, y_flip)\n",
    "sum(y_flip == logreg.predict(X))\n",
    "\n",
    "99\n",
    "\n",
    "Cool - they also get 99 / 100 correct. Looking good :)\n",
    "Advanced Optimization\n",
    "\n",
    "So gradient descent is one way to learn our β\n",
    "\n",
    "values, but there are some other ways too. Basically these are more advanced algorithms that I won't explain, but that can be easily run in Python once you have defined your cost function and your gradients. These algorithms are:\n",
    "\n",
    "    BFGS\n",
    "        http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html\n",
    "    L-BFGS: Like BFGS but uses limited memory\n",
    "        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html\n",
    "    Conjugate Gradient\n",
    "        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html\n",
    "\n",
    "Here are the very high level advantages / disadvantages of using one of these algorithms over gradient descent:\n",
    "\n",
    "    Advantages\n",
    "        Don't need to pick learning rate\n",
    "        Often run faster (not always the case)\n",
    "        Can numerically approximate gradient for you (doesn't always work out well)\n",
    "    Disadvantages\n",
    "        More complex\n",
    "        More of a black box unless you learn the specifics\n",
    "\n",
    "The one I hear most about these days is L-BFGS, so I will use it as my example. To use the others, all you do is replace the scipy function with the one in the links above. All the arguments remain the same. Also, I will now use all 4 features as opposed to just 2.\n",
    "L-BFGS\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "#normalize data\n",
    "norm_X = (X_full - np.mean(X_full, axis=0)) / np.std(X_full, axis=0)\n",
    "myargs = (norm_X, y_flip)\n",
    "betas = np.zeros(norm_X.shape[1])\n",
    "lbfgs_fitted = fmin_l_bfgs_b(cost_func, x0=betas, args=myargs, fprime=log_gradient)\n",
    "lbfgs_fitted[0]\n",
    "\n",
    "array([ -1.39630462,   5.3512917 ,  -9.41860088, -10.84876254])\n",
    "\n",
    "Above are the β\n",
    "\n",
    "values we have learned. Now let's make some predictions.\n",
    "\n",
    "lbfgs_predicted = pred_values(lbfgs_fitted[0], norm_X, hard=True)\n",
    "sum(lbfgs_predicted == y_flip)\n",
    "\n",
    "100\n",
    "\n",
    "A perfect 100 - not bad.\n",
    "Compare with Scikit-Learn\n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(norm_X, y_flip)\n",
    "sum(y_flip == logreg.predict(norm_X))\n",
    "\n",
    "100\n",
    "\n",
    "Compare with our implementation\n",
    "\n",
    "fitted_values, cost_iter = grad_desc(betas, norm_X, y_flip)\n",
    "predicted_y = pred_values(fitted_values, norm_X)\n",
    "sum(predicted_y == y_flip)\n",
    "\n",
    "100\n",
    "\n",
    "So with all 4 features we all get a perfect accuracy, which is to be expected given that the classes are linearlly seperable. So no surprise here, but it is nice to know things are working :). Note: This example doesn't really let L-BFGS shine. The purpose of this post, though, isn't to evaluate advanced optimization techniques. If this is your interest try running some tests with much larger data with many more features and less seperable classes.\n",
    "Conclusion\n",
    "\n",
    "I hope this little tutorial helped you understand in some depth logistic regression. It is a powerful tool that is good to know. It can even become more powerful with things like regularization.\n",
    "\n",
    "Even more so, I hope this helped explain the steps of how a learning algorithm might be designed. Having a grasp on what a cost function is and how to minimize it with techniques such as gradient descent can really help understand some of the machine learning literature.\n",
    "\n",
    "Anyway - if you have any questions or comments. I would love to hear them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
